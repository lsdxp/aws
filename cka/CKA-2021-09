kubectl config use-context your-k8s-name

1.
kubectl create ns app-team1
kubectl create clusterrole deployment-clusterrole --resource=deployment,statefulset,daemonset --verb=create -n app-team1
kubectl create sa cicd-token -n app-team1
kubectl create rolebinding cicd-rolebinding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token -n app-team1

2.
kubectl cordon ek8s-node-1
kubectl drain ek8s-node-1 --ignore-daemonsets --delete-emptydir-data

3.
ssh mk8s-master-0
kubectl cordon mk8s-master-0
sudo -i
kubectl drain mk8s-master-0 --ignore-daemonsets

apt-mark unhold kubeadm kubectl kubelet && apt-get update && apt-get install -y kubeadm=1.21.x-00 kubelet=1.21.x-00 kubectl=1.21.x-00 && \
apt-mark hold kubeadm kubectl kubelet
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.21.x --etcd-upgrade=false
sudo systemctl daemon-reload
sudo systemctl restart kubelet
kubectl uncordon mk8s-master-0


*4.

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> snapshot save /var/lib/backup/etcd-snapshot.db

ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup snapshot restore /data/backup/etcd-snapshot-previous.db

edit etcd.yml volume of etcd-data to use /data/backup/etcd-snapshot-previous.db

volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
    
    
5.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          namespace: my-app
      podSelector:
        matchLabels: {}
    ports:
    - protocol: TCP
      port: 80

6.

        ports:
        - name: http
          protocol: TCP
          containerPort: 80

kubectl expose deployment front-end --type=NodePort --name=front-end-svc --targetPort=80 --port=80


7.

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 5678
              

8.
kubectl scale deployment/guestbook --replicas=10

9.
apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc0041
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disk: ssd
    
10.
kubectl get nodes
kubectl describe nodes | grep -i taint | grep -i noschedule

*11.
apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc0041
spec:
  containers:
  - name: nginx
    image: nginx
  - name: redis
    image: redis

12.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
  labels:
    type: local
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/srv/app-config"
    
*13.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  storageClassName: csi-hostath-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
 
 
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-volume
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage 
          

kubectl edit pvc pv-volume --record

14.
kubectl logs foobar | grep "asdgasdg" > sadfasd


*15.
  - name: sidecar
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/11-factor-app.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
    
16.
kubectl top pod -l name=cpu-loader --sort-by=cpu -A

*17.

sudo -i 
systemctl status kubelet
systemctl enable kubelet
systemctl restart kubelet
systemctl status kubelet

