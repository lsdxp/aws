structure:

master:
kube-apiserver 
etcd
kube controller
kube-scheduler

worker:
kube-let
kube-proxy
docker/container runtime

kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 

kubeadm doesn't install kubelet

pod yaml:

apiversion
kind
metadata -> dict

spec -> list

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

commands: 
kubectl run nginx --image=nginx
kubectl describe pod pod-name
kubectl get pods -o wide
kubectl run redis --image=redis123 --dry-run=client -o yaml > pod.yml
kubectl apply -f pod.yml
kubectl edit pod redis
____________

web-service.app.svc.cluster.local

corefile -> configmaps


--------------
replication controller vs replica set

replica set -> selector and matchlabels

kubectl replace -f replicaset-definition.yml
kubectl scale --replicas=6 -f replicaset-definition.yml
kubectl scale --replicas=6 replicaset myapp-replicaset
kubectl get replicaset
kubectl delete replicaset myapp-replicaset

---------
kind: Deployment
kubectl get deployments
kubectl get all

https://kubernetes.io/docs/reference/kubectl/conventions/


-----
namespace

kubectl config set-context $(kubectl config current-context)  --namespace=dev
kubectl get pods --all-namespaces
kubectl get pods --namespace=dev

db-service.dev.svc.cluster.local (pod namespace service domain)

------
service

Nodeport, clusterIP, load balancer
kubectl expose deployment simple-webapp-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml

---------
scheduler
spec->nodeName 
-------
label
kubectl get pods --selector app=App1
kubectl get pods --selector bu=finance,env=prod,tier=frontend

----
taint effect: NoSchedule|PreferNoSchedule|NoExecute
kubectl taint nodes node1 app=blue:NoSchedule
untaint: kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

----
nodeSelector
kubectl label nodes node01 size=Large

node affinity

requiredDuringSchedulingIgnoredExecution
kubectl get deployment.apps blue -o yaml > pod.yml

--------
dameon set, one pod each node

-----
static pod

/etc/kubenetes/manifes/
--pod-manifest-path
--config -> staticPodPath

-------
multi scheduler
scheduleName in pod config
-----
https://github.com/kodekloudhub/kubernetes-metrics-server.git
kubectl top node
------
kubectl logs -f pod-name (container name if multiple)

------
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment

------
docker run --entrypoint sleep2.0 ubuntu-sleeper 10

under container
ENTRYPOINT-> command:
                - sleep2.0
                - "1200"
CMD-> args:
        - "10"

env:
   - name:
    value:
    
env:
   - name: APP_COLOR
    valueFrom:   
        configMapKeyRef:
          name: app-config
          key: APP_COLOR
envFrom:
    - configMapRef:
      name: app-config
      
volumes:
- name: app-config-volume
  configMap:
    name: app-config
kubectl create configmap app-config --from-literal=APP_COLOR=blue
kubectl create configmap app-config --from-file=app_config.properties
kubectl create -f config-map.yaml
kubectl get configmaps

kubectl create secret generic app-secret --from-literal=DB_HOST=mysql
kubectl create secret generic app-secret --from-file=app_secret.properties
kubectl create -f app-secret.yaml

echo -n 'mysql' | base64
kubectl get secrets

echo -n 'bX1zcWw-' | base64 --decode

volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret

envFrom:
- secretRef:
            name: db-secret
            
---------
There are 3 common patterns, when it comes to designing multi-container PODs. 
The first and what we just saw with the logging service example is known as a side car pattern. 
The others are the adapter and the ambassador pattern.

------------
initContainers

------
cluster maintainence

kubectl drain node01
kubectl cordon node01
kubectl uncordon node01

kubeadm upgrade plan
-----------

On the controlplane node, run the command run the following commands:

apt update
This will update the package lists from the software repository.

apt install kubeadm=1.20.0-00
This will install the kubeadm version 1.20

kubeadm upgrade apply v1.20.0
This will upgrade kubernetes controlplane. Note that this can take a few minutes.

apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

You may need to restart kubelet after it has been upgraded.
Run: systemctl restart kubelet

--------------------------------
If you are on the master node, run ssh node01 to go to node01


apt update
This will update the package lists from the software repository.


apt install kubeadm=1.20.0-00
This will install the kubeadm version 1.20


kubeadm upgrade node
This will upgrade the node01 configuration.


apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.


You may need to restart kubelet after it has been upgraded.
Run: systemctl restart kubelet

----------
ETCDCTL_API=3 etcdctl snapshot save snapshot-pre-boot.db --endpoints=https://[127.0.0.1]:2379 \
 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
 --cert=/etc/kubernetes/pki/etcd/server.crt \
 --key=/etc/kubernetes/pki/etcd/server.key \
 
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db
-------
openssl x509 -in file-path.crt -text -noout -in /etc/kubernetes/pki/apiserver.crt 

------

kubectl get csr jane -o yaml
kubectl certificate approve jane

----
kubectl config view
kubectl config use-context produser@production
kubectl config current-context

--------
kubectl proxy
curl http://localhost:8001 -k
______
node abac rbac webhook
kubectl auth can-i delete deployments --as dev-user --namespace dev


To create a Role:- kubectl create role developer --namespace=default --verb=list,create --resource=pods
To create a RoleBinding:- kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
OR



Solution manifest file to create a role and rolebinding in the default namespace:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
_____
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: blue
  name: deploy-role
rules:
- apiGroups: ["apps", "extensions"]
  resources: ["deployments"]
  verbs: ["create"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-deploy-binding
  namespace: blue
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: deploy-role
  apiGroup: rbac.authorization.k8s.io
_______
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
After save into a file, run the command kubectl create -f <file-name>.yaml to create a resources from definition file.
-______
Solution manifest file to create a clusterrole and clusterrolebinding for michelle user:

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io
After save into a file, run the command kubectl create -f <file-name>.yaml to create a resources from definition file.
https://kodekloud.com/topic/practice-test-cluster-roles-2/?utm_source=udemy&utm_medium=labs&utm_campaign=kubernetes

_________
kubectl create serviceaccount dashboard-sa

registry->


kubectl create secret docker-registry private-reg-cred --docker-server=myprivateregistry.com:5000 --docker-username=dock_user --docker-password=dock_password --docker-email=dock_user@myprivateregistry.com

securityContext:
  runAsUser: 1010
——————

kubectl get networkpolicy

podSelector
namesapceSelector
ipBlock

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
---------
storage

docker volume create data_volume
#docker run -v data_volume:/var/lib/mysql mysql
#docker run -v /data/mysql:/var/lib/mysql mysql
docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql
docker run -it --name mysql --volume-driver rexray/ebs --mount src=ebs-vol,target=/var/lib/mysql mysql

kubectl get persistentvolume
kubectl get persistentvolumeclaim

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
-------
root@controlplane:~# kubectl exec webapp -- cat /log/app.log

static provisioning / dynamic provisioning
storage class
-----
ip link/ip a
ip route
ifconfig
netstat -nplt / netstat -natulp
netstat -anp

______

get node ip range: 
ip a

get pod ip range:
kubectl -n kube-system logs weave-net-4x768 weave  

get service ip range:

    - --service-cluster-ip-range=10.96.0.0/12

get kube-proxy setup:
kubectl -n kube-system logs kube-proxy-ff6qd kube-proxy

daemon set

-----------



